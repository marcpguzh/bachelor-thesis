{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbd9513-9f90-41fd-8ff0-5466af630583",
   "metadata": {},
   "source": [
    "# Datenextraktion\n",
    "#### Marc Peter Gr√ºniger\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbefee04-e585-4b03-9675-b66ab5a82b23",
   "metadata": {},
   "source": [
    "## Bibliotheken & Plot-Einstellungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f86b634-2fc6-43be-b110-d435e4665390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System & Standardbibliotheken\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "# Drittanbieterbibliotheken\n",
    "import requests\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# LlamaIndex ‚Äì Core\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    QueryBundle\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "# LlamaIndex ‚Äì LLM & Parser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_parse import LlamaParse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e64ed3-98d9-419a-b200-9665418860b6",
   "metadata": {},
   "source": [
    "## API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ac4be89-1460-40b2-bae1-e3542635a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# API-Keys laden\n",
    "llama_cloud_api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\").rstrip(\"/\")\n",
    "azure_embedding_deployment = os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\")\n",
    "azure_api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfe8a5f-56f0-4b3a-8164-28440adcdcb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13748d25-400f-433f-8332-721ef441904a",
   "metadata": {},
   "source": [
    "## Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be22956a-2aaa-44aa-9ea2-351f34a93362",
   "metadata": {},
   "source": [
    "### Prompts laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3b5bcbc-2162-4ac9-a37f-c46f978aed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(prompt_name: str) -> str:\n",
    "    \"\"\"\n",
    "    L√§dt den Inhalt einer Prompt-Datei aus dem Verzeichnis 'prompts'.\n",
    "\n",
    "    Args:\n",
    "        prompt_name (str): Der Name der Prompt-Datei (ohne Dateiendung '.txt').\n",
    "\n",
    "    Returns:\n",
    "        str: Der geladene Prompt-Text als String.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    prompt_path = os.path.join(\"prompts\", f\"{prompt_name}.txt\")\n",
    "    with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44afff-0096-4f99-864c-9666262de6bf",
   "metadata": {},
   "source": [
    "### Extrahierte Werte speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79c2b7-31ec-410d-833e-095181cd72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_data_json(pdf_path, key, value):\n",
    "    \"\"\"\n",
    "    Speichert einen extrahierten Wert in eine zentrale JSON-Datei, gruppiert nach Pensionskassenname.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Pfad zur PDF-Datei, aus dem der Pensionskassenname abgeleitet wird.\n",
    "        key (str): Der Name des Datenpunkts, der gespeichert werden soll.\n",
    "        value (Any): Der extrahierte Wert, der gespeichert werden soll.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Wenn der PDF-Pfad nicht gen√ºgend Informationen enth√§lt, um den Pensionskassennamen zu bestimmen.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pfad aufteilen und den Ordnernamen vor der PDF-Datei als Pensionskassenname verwenden\n",
    "    parts = pdf_path.replace(\"\\\\\", \"/\").split(\"/\")\n",
    "    if len(parts) < 2:\n",
    "        raise ValueError(\"PDF-Pfad ist zu kurz, um den Pensionskassennamen zu extrahieren.\")\n",
    "    pk_name = parts[-2]\n",
    "\n",
    "    json_path = \"../0. Daten/extrahierte_daten.json\"\n",
    "\n",
    "\n",
    "    # Vorhandene Daten laden oder neues Dict anlegen\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    # Neuen Eintrag f√ºr die Pensionskasse erstellen, falls noch nicht vorhanden\n",
    "    if pk_name not in data:\n",
    "        data[pk_name] = {}\n",
    "\n",
    "    # Datenpunkt hinzuf√ºgen oder aktualisieren\n",
    "    data[pk_name][key] = value\n",
    "\n",
    "    # JSON-Datei speichern\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Wert f√ºr '{key}' unter '{pk_name}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3437f-6d3d-4f27-b287-504bdae327d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3faf6-b25e-4be5-9692-47b520c70bb2",
   "metadata": {},
   "source": [
    "## 1. Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5986b-b24f-4e09-b274-0658fdb21915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ÑπÔ∏è Keine neuen PDF-Dateien mussten geparst werden ‚Äì alles bereits vorhanden.\n"
     ]
    }
   ],
   "source": [
    "# Vorbereitung f√ºr Jupyter-Notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# === Parsing-Konfiguration ===\n",
    "\n",
    "# Sicherstellen, dass der API-Key f√ºr LlamaParse geladen wurde\n",
    "if not \"llama_cloud_api_key\" in globals() or not llama_cloud_api_key:\n",
    "    raise ValueError(\"Die Variable 'llama_cloud_api_key' ist nicht definiert. Bitte API-Keys zuvor laden.\")\n",
    "\n",
    "# Pfad zur obersten Ordnerstruktur mit den PDF-Berichten\n",
    "input_root = \"pensionskassen gb\"\n",
    "\n",
    "# Relevante PDF-Dateinamen, die extrahiert werden sollen\n",
    "target_filenames = {\"GB.pdf\", \"GB_FR.pdf\", \"GB_IT.pdf\"}\n",
    "\n",
    "# Funktion zur Verarbeitung einer einzelnen PDF-Datei mit LlamaParse\n",
    "async def parse_single_pdf(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    Parst eine einzelne PDF-Datei mithilfe von LlamaParse in Markdown-Text\n",
    "    und speichert das Ergebnis in einer .md-Datei.\n",
    "    \"\"\"\n",
    "    parser = LlamaParse(api_key=llama_cloud_api_key, result_type=\"markdown\")\n",
    "    reader = SimpleDirectoryReader(input_files=[pdf_path], file_extractor={\".pdf\": parser})\n",
    "    documents = await reader.aload_data()\n",
    "\n",
    "    if not documents:\n",
    "        print(f\"‚ö†Ô∏è Kein Text extrahiert: {pdf_path}\")\n",
    "        return False\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for doc in documents:\n",
    "            f.write(doc.text + \"\\n\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Markdown gespeichert: {output_path}\")\n",
    "    return True\n",
    "\n",
    "# Funktion zur Verarbeitung aller relevanten PDFs\n",
    "async def parse_pdfs():\n",
    "    \"\"\"\n",
    "    Durchsucht die Ordnerstruktur nach relevanten PDFs und erzeugt Markdown-Dateien,\n",
    "    sofern sie noch nicht vorhanden sind.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    any_parsed = False  # Marker, ob etwas verarbeitet wurde\n",
    "\n",
    "    for root, _, files in os.walk(input_root):\n",
    "        for file in files:\n",
    "            if file in target_filenames:\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                md_path = os.path.join(root, \"Text.md\")\n",
    "\n",
    "                # √úberspringe Datei, wenn bereits Markdown vorhanden ist\n",
    "                if os.path.exists(md_path):\n",
    "                    continue\n",
    "\n",
    "                # Async-Task zur Verarbeitung hinzuf√ºgen\n",
    "                tasks.append(parse_single_pdf(pdf_path, md_path))\n",
    "\n",
    "    # F√ºhre alle Tasks parallel aus\n",
    "    if tasks:\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        any_parsed = any(results)\n",
    "\n",
    "    # Abschluss-Info\n",
    "    if any_parsed:\n",
    "        print(\"\\nüéâ Alle fehlenden PDF-Dateien wurden erfolgreich geparst.\")\n",
    "    else:\n",
    "        print(\"\\n‚ÑπÔ∏è Keine neuen PDF-Dateien mussten geparst werden ‚Äì alles bereits vorhanden.\")\n",
    "\n",
    "# Starte den Parsing-Prozess\n",
    "await parse_pdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7ce60-5a86-43f4-978d-bdc2a260a7dd",
   "metadata": {},
   "source": [
    "## 2. Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fad35ecb-ed7a-4578-8ed8-461d39d0c911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Keine neuen Indizes erforderlich ‚Äì alle bereits vorhanden.\n"
     ]
    }
   ],
   "source": [
    "# Initialisiere den SentenceSplitter zur Textzerlegung\n",
    "parser = SentenceSplitter(chunk_size=512, chunk_overlap=64)\n",
    "\n",
    "# Marker, ob neue Indizes erstellt wurden\n",
    "any_index_created = False\n",
    "\n",
    "# Durchlaufe alle Unterordner im input_root-Verzeichnis\n",
    "for root, _, files in os.walk(input_root):\n",
    "    if \"Text.md\" not in files:\n",
    "        continue  # √úberspringen, wenn keine Text.md vorhanden ist\n",
    "\n",
    "    storage_path = os.path.join(root, \"index_storage\")\n",
    "\n",
    "    # Falls der Index bereits vorhanden ist, √ºberspringen\n",
    "    if os.path.exists(storage_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÇ Erzeuge Index (ohne Embedding) f√ºr: {root}\")\n",
    "\n",
    "    # Lade das Text.md-Dokument\n",
    "    docs = SimpleDirectoryReader(input_files=[os.path.join(root, \"Text.md\")]).load_data()\n",
    "\n",
    "    # Zerlege das Dokument in TextNodes mit automatischer Metadatenerkennung\n",
    "    nodes = parser.get_nodes_from_documents(docs)\n",
    "\n",
    "    # Erzeuge den Index ohne Embedding-Modell (reine Struktur & Metadaten)\n",
    "    index = VectorStoreIndex(nodes, embed_model=None)\n",
    "\n",
    "    # Speichere den Index dauerhaft im jeweiligen Ordner\n",
    "    index.storage_context.persist(persist_dir=storage_path)\n",
    "\n",
    "    print(f\"‚úÖ Index gespeichert mit Metadaten (ohne Embeddings): {os.path.basename(root)}\")\n",
    "    any_index_created = True\n",
    "\n",
    "# Abschluss-Info\n",
    "if not any_index_created:\n",
    "    print(\"‚ÑπÔ∏è Keine neuen Indizes erforderlich ‚Äì alle bereits vorhanden.\")\n",
    "else:\n",
    "    print(\"üéâ Neue Indizes wurden erfolgreich erstellt und gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30f6e8-ba16-472a-871a-d255ffab90e5",
   "metadata": {},
   "source": [
    "## 3. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7a6ecb07-2825-4786-986d-c0203ff775e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Keine neuen Embeddings erforderlich ‚Äì alle bereits vorhanden.\n"
     ]
    }
   ],
   "source": [
    "# Azure Embedding-Funktion\n",
    "def get_azure_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Sendet eine Liste von Texten an die Azure OpenAI API und gibt Embeddings zur√ºck.\n",
    "    \"\"\"\n",
    "    url = f\"{azure_endpoint}/openai/deployments/{azure_embedding_deployment}/embeddings?api-version={azure_api_version}\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": azure_api_key}\n",
    "    data = {\"input\": texts, \"model\": \"text-embedding-3-large\"}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        return [entry[\"embedding\"] for entry in response.json()[\"data\"]]\n",
    "    elif response.status_code == 429:\n",
    "        print(\"‚ö†Ô∏è Rate Limit ‚Äì warte 10 Sekunden...\")\n",
    "        time.sleep(10)\n",
    "        return get_azure_embeddings(texts)\n",
    "    else:\n",
    "        raise ValueError(f\"‚ùå Fehler bei Embedding: {response.status_code} - {response.text}\")\n",
    "\n",
    "# === Verarbeitung: Embeddings hinzuf√ºgen und speichern ===\n",
    "any_embedded = False  # Marker, ob √ºberhaupt neue Embeddings generiert wurden\n",
    "\n",
    "for root, _, files in os.walk(input_root):\n",
    "    storage_path = os.path.join(root, \"index_storage\")\n",
    "    if not os.path.exists(storage_path):\n",
    "        continue  # Kein Index vorhanden ‚Üí √ºberspringen\n",
    "\n",
    "    # Index laden\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storage_path)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "    # Nodes laden\n",
    "    docstore = storage_context.docstore\n",
    "    vector_store = storage_context.vector_store\n",
    "    all_nodes = list(docstore.docs.values())\n",
    "\n",
    "    # Check: Gibt es bereits Embeddings?\n",
    "    already_embedded = [\n",
    "        node.node_id in vector_store._data.embedding_dict\n",
    "        for node in all_nodes\n",
    "    ]\n",
    "\n",
    "    if all(already_embedded):\n",
    "        continue  # √úberspringen\n",
    "\n",
    "    # Embeddings generieren\n",
    "    texts = [node.text for node in all_nodes]\n",
    "    embeddings = get_azure_embeddings(texts)\n",
    "\n",
    "    if embeddings is None or len(embeddings) != len(all_nodes):\n",
    "        print(f\"‚ùå Fehler ‚Äì Anzahl Embeddings stimmt nicht\")\n",
    "        continue\n",
    "\n",
    "    # Embeddings zuweisen\n",
    "    for node, emb in zip(all_nodes, embeddings):\n",
    "        node.embedding = emb\n",
    "\n",
    "    # Speichern\n",
    "    storage_context.vector_store.add(all_nodes)\n",
    "    storage_context.persist(persist_dir=storage_path)\n",
    "\n",
    "    print(f\"‚úÖ Embeddings gespeichert f√ºr: {os.path.basename(root)}\")\n",
    "    any_embedded = True\n",
    "\n",
    "# Hinweis am Schluss, wenn nichts gemacht wurde\n",
    "if not any_embedded:\n",
    "    print(\"‚ÑπÔ∏è Keine neuen Embeddings erforderlich ‚Äì alle bereits vorhanden.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fb229-af28-41a6-a43f-e9d4333bacfd",
   "metadata": {},
   "source": [
    "## 4. Querying-Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d34b6957-7ffa-47a1-8313-4bc7d54b179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding-Funktion √ºber Azure\n",
    "def get_query_embedding(prompt):\n",
    "    url = f\"{azure_endpoint}/openai/deployments/{azure_embedding_deployment}/embeddings?api-version={azure_api_version}\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": azure_api_key}\n",
    "    data = {\"input\": [prompt], \"model\": \"text-embedding-3-large\"}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"data\"][0][\"embedding\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Fehler bei Embedding: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79f150-ec23-4824-bc61-943109085cd9",
   "metadata": {},
   "source": [
    "## 5. Prompting + Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bdc50f4c-97bc-42f3-b99e-b9efd7526690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der Kennzahlen f√ºr die Extraktion\n",
    "# Jede Kennzahl besteht aus:\n",
    "# - \"key\": Name f√ºr die JSON-Speicherung\n",
    "# - \"query\": Suchbegriff f√ºr semantische Embedding-Suche im PDF\n",
    "# - \"prompt\": spezifische GPT-Aufforderung zur Antwort\n",
    "\n",
    "metrics = {\n",
    "    \"verm√∂gensverwaltungskosten\": {\n",
    "        \"key\": \"Verm√∂gensverwaltungskosten\",\n",
    "        \"query\": \"Verm√∂gensverwaltungskosten per 31.12.2023 ‚Äì Verwaltungsaufwand der Verm√∂gensanlagen, Kostenquote, Ausweis in % der transparenten Verm√∂gensanlagen, TER, OAK\",\n",
    "        \"prompt\": load_prompt(\"vvk\")\n",
    "    },\n",
    "    \"bilanzsumme\": {\n",
    "        \"key\": \"Bilanzsumme\",\n",
    "        \"query\": \"Bilanzsumme per 31.12.2023 ‚Äì Total Aktiven, Gesamtverm√∂gen, Summe Aktiven laut Jahresrechnung oder Bilanz\",\n",
    "        \"prompt\": load_prompt(\"bilanzsumme\")\n",
    "    },\n",
    "    \"aktienquote\": {\n",
    "        \"key\": \"Total Aktien (%)\",\n",
    "        \"query\": \"Aktienquote per 31.12.2023 ‚Äì Aktien Schweiz, Ausland, Total Aktien laut Verm√∂gensaufteilung nach Anlagekategorien\",\n",
    "        \"prompt\": load_prompt(\"aktienquote\")\n",
    "    },\n",
    "    \"immobilienquote\": {\n",
    "        \"key\": \"Total Immobilien (%)\",\n",
    "        \"query\": \"Immobilienquote per 31.12.2023 ‚Äì Immobilien Schweiz, Ausland, Total Immobilien laut Verm√∂gensaufteilung nach Anlagekategorien\",\n",
    "        \"prompt\": load_prompt(\"immobilienquote\")\n",
    "    },\n",
    "    \"nominalwertquote\": {\n",
    "        \"key\": \"Total Nominalwerte (%)\",\n",
    "        \"query\": \"Nominalwertquote per 31.12.2023 ‚Äì Obligationen, Hypotheken, Liquidit√§t laut Verm√∂gensaufteilung nach Anlagekategorien\",\n",
    "        \"prompt\": load_prompt(\"nominalwertquote\")\n",
    "    },\n",
    "    \"alternativquote\": {\n",
    "        \"key\": \"Total Alternative Anlagen (%)\",\n",
    "        \"query\": \"Alternative Anlagen per 31.12.2023 ‚Äì Private Equity, Hedgefonds, Infrastruktur laut Verm√∂gensaufteilung nach Anlagekategorien\",\n",
    "        \"prompt\": load_prompt(\"alternativquote\")\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960450a7-56a8-48a4-b2a8-b79df0f89b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Stelle GPT-gest√ºtzte semantische Abfrage an einen Index\n",
    "def query_pensionskasse(path_to_index, retrieval_query, user_prompt, top_k=5):\n",
    "    \"\"\"\n",
    "    F√ºhrt eine Retrieval-Augmented Generation (RAG) durch:\n",
    "    1. Findet relevante Textstellen im Index basierend auf einem semantischen Suchbegriff (Query).\n",
    "    2. Stellt eine gezielte Frage an GPT-4o mit diesem Kontext.\n",
    "\n",
    "    Args:\n",
    "        path_to_index (str): Pfad zum Ordner mit dem gespeicherten LlamaIndex.\n",
    "        retrieval_query (str): Semantischer Suchbegriff f√ºr die Kontextsuche.\n",
    "        user_prompt (str): Konkrete GPT-Aufforderung.\n",
    "        top_k (int): Anzahl der Top-Kontextabschnitte f√ºr die GPT-Antwort (Default: 5).\n",
    "\n",
    "    Returns:\n",
    "        str: Die Antwort von GPT-4o als Klartext.\n",
    "    \"\"\"\n",
    "    # 1. Index & Retriever laden\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=path_to_index)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "\n",
    "    # 2. Embedding f√ºr die semantische Suche erzeugen\n",
    "    retrieval_embedding = get_query_embedding(retrieval_query)\n",
    "    query_bundle = QueryBundle(query_str=retrieval_query, embedding=retrieval_embedding)\n",
    "    nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    # 3. Kontext aus den gefundenen Textstellen erstellen\n",
    "    context = \"\\n\\n\".join([node.text.strip() for node in nodes])\n",
    "    full_prompt = f\"Kontext:\\n{context}\\n\\nPrompt:\\n{user_prompt}\"\n",
    "\n",
    "    # 4. Anfrage an GPT-4o stellen\n",
    "    llm = OpenAI(model=\"gpt-4o\", api_key=openai_api_key)\n",
    "    response = llm.complete(full_prompt)\n",
    "\n",
    "    # 5. Ausgabe f√ºr Debug-Zwecke\n",
    "    pk_name = path_to_index.rstrip(\"/\").split(\"/\")[-1]\n",
    "    print(f\"üîç {pk_name} ‚Üí {retrieval_query}\\nüß† GPT-4o antwortet:\\n\\n{response.text.strip()}\")\n",
    "\n",
    "    return response.text.strip()\n",
    "\n",
    "\n",
    "# Funktion: Extrahiere alle definierten Metriken f√ºr alle Pensionskassen im Verzeichnis\n",
    "def extract_all_metrics(root_dir=\"pensionskassen gb\"):\n",
    "    \"\"\"\n",
    "    F√ºhrt f√ºr alle vorhandenen Indexe im Zielverzeichnis eine automatische Extraktion der Metriken durch.\n",
    "    Die Ergebnisse werden in einer JSON-Datei pro PDF gespeichert.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Hauptordner mit allen Pensionskassen-Unterordnern und jeweiligen Indexen.\n",
    "    \"\"\"\n",
    "    for folder in os.listdir(root_dir):\n",
    "        pk_dir = os.path.join(root_dir, folder)\n",
    "        index_dir = os.path.join(pk_dir, \"index_storage\")\n",
    "\n",
    "        if not os.path.isdir(index_dir):\n",
    "            continue  # Kein g√ºltiger Index vorhanden ‚Üí √ºberspringen\n",
    "\n",
    "        for metric_id, metric in metrics.items():\n",
    "            try:\n",
    "                print(f\"üîç {folder} ‚Üí {metric['key']}\")\n",
    "                \n",
    "                # 1. Semantische Suche & GPT-Antwort\n",
    "                response = query_pensionskasse(\n",
    "                    path_to_index=index_dir,\n",
    "                    retrieval_query=metric[\"query\"],\n",
    "                    user_prompt=metric[\"prompt\"],\n",
    "                    top_k=5\n",
    "                )\n",
    "\n",
    "                # 2. Extraktion des numerischen Werts aus der GPT-Antwort\n",
    "                value_raw = str(response)\n",
    "                value = float(value_raw.replace(\"'\", \"\").replace(\" \", \"\").replace(\",\", \"\").replace(\"‚Äô\", \"\"))\n",
    "\n",
    "                # 3. Zuordnen zum richtigen PDF (Deutsch, FR oder IT)\n",
    "                pdf_path = os.path.join(pk_dir, \"GB.pdf\")\n",
    "                if not os.path.exists(pdf_path):\n",
    "                    for alt in [\"GB_FR.pdf\", \"GB_IT.pdf\"]:\n",
    "                        alt_path = os.path.join(pk_dir, alt)\n",
    "                        if os.path.exists(alt_path):\n",
    "                            pdf_path = alt_path\n",
    "                            break\n",
    "\n",
    "                if not os.path.exists(pdf_path):\n",
    "                    print(f\"‚ö†Ô∏è Kein PDF gefunden f√ºr {folder}\")\n",
    "                    continue\n",
    "\n",
    "                # 4. Ergebnis speichern\n",
    "                save_to_data_json(pdf_path, metric[\"key\"], value)\n",
    "                print(f\"‚úÖ {metric['key']}: {value}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Fehler bei {folder} ‚Üí {metric['key']}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0d4b59e6-5529-4a07-bd72-2da2245c6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Finaler Abruf der Extraktionsfunktion ===\n",
    "# ‚ùó Hinweis: Auskommentiert, da jeder Abruf GPT-4o verwendet und somit API-Kosten verursacht.\n",
    "# ‚ùó Nur ausf√ºhren, wenn alle Indizes und Embeddings bereit sind.\n",
    "\n",
    "# extract_all_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
